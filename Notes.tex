\documentclass{article}
\usepackage[utf8x]{inputenc} 
\usepackage[ngerman]{babel} 
\usepackage{amsmath} 
\usepackage{caption} 
\usepackage{xcolor} 
\usepackage{listings} 
\lstset{
   basicstyle   = \footnotesize\ttfamily,
   breaklines   = true,
   commentstyle = \color{blue},
   keywordstyle = \color{purple}\textbf,
   numberstyle  = \tiny\color{gray},
   numbers      = left,
   stringstyle  = \color{olive},
}

\title{FPTAS für das \emph{Restricted Shortest Path}-Problem}
\author{Rasmus Diederichsen \and Sebastian Höffner}

\begin{document}
\maketitle
\section{Problemstellung}

Das \emph{Shortest Path}-Problem besteht darin, in einem gewichteten Graphen den
kürzesten Weg von einem Start- zum Zielknoten auszurechnen. Wir kennen schon das
\emph{Single Source Shortest Path}-Problem, das darin besteht vom Startknoten
die kürzesten Wege zu allen anderen Knoten auszurechnen. Das Problem lässt sich
mit Dijkstras Algorithmus lösen. Weiterhin gibt es das \emph{All Pairs Shortest
Path}-Problem, für das wir Floyds Algorithmus kennengelernt haben. Hierbei
werden alle kürzesten Wege zwischen je zwei Knoten berechnet.

Unser Problem wandelt die obigen insofern ab, als Kanten nicht mehr nur Gewicht
$c_{ij}$ haben, sonden nun noch eine Verzögerung (\emph{delay} oder
\emph{transit time}) $t_{ij}$ zusätzlich haben.

Wir möchten den nach Gewichten kürzesten Pfad von Knoten $1$ nach $n$ berechnen,
dessen Verzögerung $\le T$ ist.

\section{Exakte Lösung}

Durch dynamische Programmierung kann das Problem gelöst werden.
Sei $g_j(c)$ die Gesamtverzögerund des schnellsten Pfades von $1$ zu $j$, der
nicht länger als $c$ ist. Eine vereinfachende Annahme ist, dass der Graph
azyklisch ist, sodass $(i,j) \in E \Rightarrow i < j$. Angeblich kann man dies
leicht auf zyklische Graphen erweitern (wie auch immer).

\subsection{Algorithmus B}

\begin{center}
   \begin{minipage}{\linewidth}
      \begin{align*}
         g_1(c) & =  0,                                                                                            & c = 0,\ldots,OPT, \\
         g_j(0) & =  \infty,                                                                                       & j = 2,\ldots,n, \\
         g_j(c) & =  \min\left\{g_j(c-1), \min_{k \mid c_{kj} \le
   c}\left\{g_k(c-c_{kj}) + t_{kj}\right\}\right\}, & j = 2,\ldots,n; ~ c= 1,\ldots,OPT
      \end{align*}
      \captionof{figure}{Algorithmus B}
   \end{minipage}
\end{center}

Die Laufzeit ist hier $\mathcal{O}(OPT\cdot n \cdot \text{Aufwand pro
$(c,j)$})$. Für jedes $g_j(c)$ kann ein Aufwand von $\mathcal{O}\left(n\right)$ anfallen,
da alle Vorgängerknoten durchsucht werden müssen. Es ergibt sich eine Laufzeit
von $\mathcal{O}\left(n^2 OPT\right) = \mathcal{O}\left(|E| OPT\right)$.

\subsection{Wann terminiert der Algorithmus?}

Man kennt zwar $OPT$ nicht, aber man weiß, dass $OPT=\min\left\{c \mid g_n(c)
\le T\right\}$, also setzt man $OPT$, sobald man das erste $c$ gefunden hat mit 
$g_n(c)\le T$, denn schneller kann es per Definition von $g$ nicht werden.

Das Problem ist, dass der Algorithmus nur pseudopolynomiell ist (vgl. Rucksack).
die Kantengewichte und damit $OPT$ können aber exponentiell in der Eingabe(bit)länge sein.

Gemäß (Lorenz et al., 1999) gilt die Komplexität nur für azyklische Graphen. Für
beliebige (auch mit Kantengewichten $0$) sei die Laufzeit
$\mathcal{O}\left(|E||V|OPT\right)$.

\section{Die Test-Prozedur}

Um ein FPTAS für das Problem zu konstruieren, wird zunächst ein Verfahren
gesucht, das untere und obere Schranken für $OPT$ berechnet. Hassin bemerkt,
dass ein wesentliches Problem bei der Entwicklung von FPTAS war, dass man keine
guten Grenzen kannte, deren Quotient polynomiell in der Eingabegröße ist.
\textcolor{red}{Wir haben allerdings keine Ahnung, warum so etwas hilfreich wäre}.

Man wünscht sich einen polynomiellen $TEST(k)$, sodass 
\begin{align*}
   TEST(k) = 
   \begin{cases}
      1 & \text{falls } OPT \ge k \\
      0 & \text{falls } OPT < k \\
   \end{cases} 
\end{align*}
denn mit diesem könnte man durch binäre Suche auf $\left\{0,\ldots,UB\right\}$ das
Problem exakt lösen. Es ist aber $NP$-schwer, daher wird ein Test gesucht,
sodass
\begin{align*}
   TEST(k) = 
   \begin{cases}
      1 & \text{falls } OPT \ge k \\
      0 & \text{falls } OPT < k(1 + \epsilon) \\
   \end{cases} 
\end{align*}

Der Test skaliert und rundet alle Kantengewichte als $\hat{c}_{ij} =
\lfloor\frac{c_{ij} (n-1)}{k\epsilon}\rfloor$ und wendet dann Algorithmus B an,
bis $g_n(c) \le T$ gefunden ist für $c < \frac{n-1}{\epsilon}$ oder $c \ge
\frac{n-1}{\epsilon}$.

Falls $c < \frac{n-1}{\epsilon}$, dann gilt für den gefundenen Pfad im
Originalgraphen
\begin{align*}
   k & \le k \\
   \frac{k\epsilon}{n-1} \frac{n-1}{\epsilon} & \le k \\
   \intertext{Es folgt}
   \frac{k\epsilon}{n-1} c & < k \\
   \frac{k\epsilon}{n-1} c + k\epsilon & < k + k\epsilon\\
   \frac{k\epsilon}{n-1} c + k\epsilon & < k(1+\epsilon)\\
\end{align*}

Falls stattdessen $c \ge \frac{n-1}{\epsilon}$, so sind die Kosten im
Originalgraphen größer gleich
\begin{align*}
   c & \ge \frac{n-1}{\epsilon} \\
   c \frac{k\epsilon}{n-1} & \ge \frac{k\epsilon}{n-1}\frac{n-1}{\epsilon} \\
   c \frac{k\epsilon}{n-1} & \ge k \\
\end{align*}

Zudem ist $g_n(c)$ der schnellste $T$-Pfad, alle anderen sind also höchstens so
schnell.  Somit leistet der Test das Geforderte.

\begin{lstlisting}[escapeinside={*}{*}]
Setze *$c \leftarrow 0$*
F*ü*r alle *$(i,j)\in E$*:
   Falls *$c_{ij}$ > k*, entferne *$(i,j)$*
   Sonst *$c_{ij} \leftarrow \lfloor c_{ij}(n-1) / k\epsilon\rfloor$*

Falls *$c \ge (n-1) / \epsilon$*, return true
Sonst: 
   Wende Algorithmus B an und berechne *$g_j(c)$* f*ü*r *$j=2,\ldots,n$*
   Falls *$g_n(c) \le T$*, return false
   Sonst:
      Setze *$c \leftarrow c + 1$*
      Goto Zeile 6
\end{lstlisting}
 Durch binäre Suche kann man nach oben beschränkte Zahlen runden, und alle
 größer als $k$ werden hier rausgeworfen. Dafür fällt Lauftzeit
 $\mathcal{O}\left(|E|\log{\frac{n-1}{\epsilon}}\right)$ an. Algorithmus B führt insgesamt
 weniger als $\frac{n-1}{\epsilon}$ Iterationen durch, für die je
 schlimmstenfalls $\mathcal{O}\left(|E|\right)$ Zeit anfällt. Die Gesamtlaufzeit des Tests
 ist daher $\mathcal{O}\left(|E|\log{\frac{n-1}{\epsilon}} + |E|\frac{n-1}{\epsilon}\right)
 = \mathcal{O}\left(|E|\frac{n-1}{\epsilon}\right)
$
\section{Das FPTAS}

Das Schema setzt obere und untere Schranken $LB \le OPT \le UB$ voraus. Initiale
Werte können z.B. $LB=1$ oder $LB=\text{kürzester Pfad nach Kosten}$ und
$UB=\sum n-1 \text{ längste Kanten}$ oder $UB=\text{Kosten des schnellsten
Pfades von $1$ nach $n$}$.

\begin{lstlisting}[escapeinside={*}{*}]
*$UB := \sum (n-1) \text{ größte Kosten}$*
*$LB := 1$*

Falls *$UB \le 2LB$*, Goto Zeile 11
Sonst:
   *$k := \sqrt{UB \cdot LB}$*
   Falls *$TEST(k)$* == true, *$LB := k$*
   Sonst *$UB := k(1 + \epsilon)$*
   Goto Zeile 4

   Setze *$c_{ij} \leftarrow c_{ij} (n-1)/LB\epsilon$*
   Berechne mit Algorithmus B die optimale L*ö*sung
\end{lstlisting}

Falls $UB \le (1+\epsilon)LB$, so hat man mit $UB$ schon eine
$\epsilon$-Approximation für $OPT$. Also sei $UB > (1+\epsilon)$. Man testet nun
mittels $TEST(k)$ für Werte $LB < k < UB(1+\epsilon)$, um die Schranken zu
verengen, da man entweder $UB=k(1+\epsilon)$ oder $LB = k$ setzen kann. Durch
eine Art binäre Suche lassen sich die Grenzen schnell einengen. Man tut dies, bis
$\frac{UB}{LB} \le 2$ oder eine andere Konstante. Die Werte für $k$ werden als
$\sqrt{UB \cdot LB}$ berechnet (\textcolor{red}{warum?}).

Um das Problem approximativ zu lösen, wendet man nun Algorithmus B auf eine skalierte
und gerundete Instanz mit Gewichten $c_{ij}(n-1)/LB\epsilon$ an. Die Abweichung
vom optimalen Pfad kann (wie für $TEST$ gezeigt) nicht größer als $k\epsilon$,
hier also $LB\epsilon < OPT\epsilon$ sein.

Durch Rundung der Kantenkosten reduziert sich die Laufzeit der letzten Anwendung
von Algorithmus B auf $\mathcal{O}\left(|E| OPT\frac{(n-1)}{LB\epsilon}\right)$. Da $OPT
\le 2LB$, ist dies eine Teilmenge von $\mathcal{O}(|E|
2LB\frac{(n-1)}{LB\epsilon})=\mathcal{O}(|E|
2\frac{(n-1)}{\epsilon})=\mathcal{O}\left(|E|\frac{n-1}{\epsilon}\right)$. Laut Hassin muss
man $\log{\log{\frac{UB}{LB}}}$ Tests durchführen, bis der Quotient auf $2$
sinkt.

Die Wurzelbestimmung kann teuer sein, aber es genügt, ein $k$ zu finden, sodass
$\sqrt[2]{\frac{UB}{LB}} < k < \sqrt{\frac{UB}{LB}}$. Dies geht offenbar in
$\log{\log{\frac{UB}{LB}}}$ (Tippfehler im Paper?). Die einzelnen Aufrufe von
$TEST$ benötigen wie oben $\mathcal{O}\left(|E|\frac{n-1}{\epsilon}\right)$. Insgesamt
ergibt sich eine Laufzeit von
$\mathcal{O}(\log\log{\frac{UB}{LB}\cdot(|E|\frac{n-1}{\epsilon} +
\log\log{\frac{UB}{LB}})})$, was polynomiell in $n$ und $\epsilon$.

% Die Wurzel exakt zu bestimmen kann teuer sein, aber man kann ein
% $k^\prime$ finden, dass größer als diw Wurzel der Wurzel, aber kleiner gleich
% die Wurzel ist in $\log{\log{\frac{UB}{LB}}}$ (Tippfehler? Paper sagt $LB/UB$
% hier). Diese Wahl von $k$ ist gut weil?

\end{document}
